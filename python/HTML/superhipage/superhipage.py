from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import Select
from selenium.common.exceptions import NoSuchElementException
import time
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin


# 1. åœ¨å…¨åŸŸå»ºç«‹ä¸¦è®€å– robots.txt
rp = RobotFileParser()
robots_url = "https://www.iyp.com.tw/robots.txt"
rp.set_url(robots_url)
rp.read()


def can_fetch_url(url: str) -> bool:
    """æª¢æŸ¥ robots.txt æ˜¯å¦å…è¨±çˆ¬èŸ²æŠ“å–é€™å€‹ URL"""
    return rp.can_fetch("*", url)


def safe_get(driver, url: str):
    """åŒ…ä¸€å±¤æª¢æŸ¥ï¼šåªæœ‰åœ¨ robots.txt å…è¨±æ™‚æ‰ driver.get"""
    if not can_fetch_url(url):
        print(f"ğŸš« robots.txt ç¦æ­¢å­˜å–ï¼š{url}")
        return False
    driver.get(url)
    return True


# Selenium Chrome é¸é …ï¼Œé—œé–‰å„ç¨® log
options = Options()
options.add_argument("--headless")
options.add_argument("--disable-gpu")
options.add_argument("--log-level=3")  # åªä¿ç•™ fatal
options.add_argument("--disable-logging")
options.add_experimental_option("excludeSwitches", ["enable-logging"])
options.add_argument("--blink-settings=imagesEnabled=false")  # ä¸è¼‰å…¥åœ–ç‰‡
options.add_argument("--disable-extensions")
options.add_argument("--no-sandbox")


# å»ºç«‹ ChromeDriver
CHROMEDRIVER_PATH = r"E:\version\Chrome_tools\chromedriver-win64\chromedriver.exe"  # <- æ”¹æˆä½ çš„å¯¦éš›ä½ç½®

#æ¸…ç†iconæ¨™ç±¤
def clean_icon_text(text):
    parts = text.strip().split(maxsplit=1)
    return parts[1] if len(parts) > 1 else text.strip()


def select_area_location():
    
    while True:
        print("è«‹é¸æ“‡é¡å‹(è¼¸å…¥:ç¸£å¸‚/å€åŸŸ):")
        select_function = input().strip()
        if select_function == "ç¸£å¸‚":
            print("è«‹è¼¸å…¥ç¸£å¸‚åç¨±(ä¸é ˆè¼¸å…¥ç¸£å¸‚ï¼Œå¦‚:å°åŒ—): ")
            location = input().strip()
            if location:
                return location
            break
        elif select_function == "å€åŸŸ":
            print("è«‹è¼¸å…¥å€åŸŸåç¨±(å…¨å€/åŒ—å€/å—å€/è¥¿å€/æ±å€): ")
            district = input().strip()
            if district:
                return district
            break
        print("âš ï¸ è¼¸å…¥æœ‰èª¤ï¼Œè«‹é‡æ–°è¼¸å…¥")


def find_area_url(driver, initial_url, area):
    if not safe_get(driver, initial_url):
        return None
    try:
        select_elem = Select(driver.find_element(By.ID, "search-range"))
        select_elem.select_by_visible_text(area)
    except NoSuchElementException:
        print(f"âŒ æ‰¾ä¸åˆ°é¸é …ï¼šã€{area}ã€ï¼Œè«‹ç¢ºèªè¼¸å…¥æ˜¯å¦æ­£ç¢º")
        return None
    value = select_elem.first_selected_option.get_attribute("value")
    search_url = f"{initial_url}&city={value}"
    return search_url


def get_area_map(driver, url):
    if not safe_get(driver, url):
        return None
    area_map = {}
    select_elem = driver.find_element(By.ID, "search-range")
    optgroups = select_elem.find_elements(By.TAG_NAME, "optgroup")
    for group in optgroups:
        label = group.get_attribute("label")
        cities = group.find_elements(By.TAG_NAME, "option")
        area_map[label] = [city.text.strip() for city in cities]
    return area_map



def Traverse_the_location_data(initial_url, location, base_url):
    # ğŸ†• æ¯æ¬¡æ–°å»º driver
    driver = webdriver.Chrome(service=Service(CHROMEDRIVER_PATH), options=options)


    # å…ˆæ‰¾ search_url
    search_url = find_area_url(driver, initial_url, location)
    if not search_url:
        driver.quit()
        exit()


    # æª¢æŸ¥ä¸¦è¼‰å…¥ search_url
    if not safe_get(driver, search_url):
        driver.quit()
        return
    
    
    output_file = f"{location}å…¬å¸è³‡æ–™åˆ—è¡¨.txt"
    with open(output_file, "w", encoding="utf-8") as f:
        page = 1
        while True:
            print(f"ğŸ” ç¬¬ {page} é ")
            
            # ç­‰å¾…å…¬å¸åç¨±å…ƒç´ å‡ºç¾
            try:
                WebDriverWait(driver, 10).until(
                    #ç”¨æ–¼éœ€ç­‰å¾…çš„å‹•æ…‹å…§å®¹  #ç­‰å¾…æ¢ä»¶æˆç«‹ 
                    EC.presence_of_element_located((By.CSS_SELECTOR, "a.text-xl"))
                    
                )
            except:
                print("âŒ ç­‰å¾…å…ƒç´ å¤±æ•—ï¼Œå¯èƒ½è³‡æ–™è¼‰å…¥ç•°å¸¸")
                break

            # æŠ“å…¬å¸å¡ç‰‡
            cards = driver.find_elements(By.CSS_SELECTOR, "div.relative.mb-4")#å›å‚³list
            if not cards:
                print("âš ï¸ æ²’æœ‰å…¬å¸è³‡æ–™ï¼ŒçµæŸ")
                break

            for card in cards:
                try:
                    name = card.find_element(By.CSS_SELECTOR, "a.text-xl").text.strip()
                    '''å»é™¤<mark>æ¨™ç±¤å…§å®¹
                    from bs4 import BeautifulSoup
                    # æŠ“åˆ° <a> å…ƒç´ 
                    a_tag = card.find_element(By.CSS_SELECTOR, "a.text-xl")
                    # å– innerHTMLï¼Œä¿ç•™ <mark> æ¨™ç±¤
                    html = a_tag.get_attribute("innerHTML")
                    # ç”¨ BeautifulSoup åˆ†æã€ç§»é™¤ <mark>
                    soup = BeautifulSoup(html, "html.parser")
                    # åˆªæ‰ <mark> æ¨™ç±¤
                    for mark in soup.find_all("mark"):
                        mark.decompose()
                    # å‰©ä¸‹çš„ç´”æ–‡å­—
                    clean_text = soup.get_text(strip=True)
                    '''
                    if not name:
                        print("éå…¬å¸è³‡æ–™")
                        break
                    # åœ°å€èˆ‡é›»è©±éƒ½åŒ…åœ¨ text-neutral-700 è£¡
                    info_block = card.find_element(By.CSS_SELECTOR, "div.text-sm.text-neutral-700")
                    details = info_block.find_elements(By.CSS_SELECTOR, "div.flex")
                    
                    addr = clean_icon_text(details[0].text) if len(details) > 0 else ""
                    tel = clean_icon_text(details[1].text) if len(details) > 1 else ""
                    if addr == "" and tel == "":
                        continue
        
                    #print(f"å…¬å¸åç¨±: {name}\nğŸ“åœ°å€: {addr}\nğŸ“é›»è©±: {tel}\n")#å°å‡ºæª¢æŸ¥é€²åº¦
                    f.write(f"å…¬å¸åç¨±: {name}\nğŸ“åœ°å€: {addr}\nğŸ“é›»è©±: {tel}\n\n")

                except Exception as e:
                    pass
                    ##rint("âš ï¸ å¡ç‰‡è™•ç†éŒ¯èª¤ï¼ˆæ›é å¯å¿½ç•¥ï¼‰:", e)

            # å˜—è©¦æ‰¾ã€Œä¸‹ä¸€é ã€é€£çµï¼ˆ<a>ï¼‰
            try:
                next_link = driver.find_element(By.XPATH, "//a[span[contains(text(),'ä¸‹ä¸€é ')]]")
                next_href = next_link.get_attribute("href")
                if not next_href:
                    print("âœ… æ²’æœ‰ä¸‹ä¸€é ï¼ŒçµæŸ")
                    break
                full_url = next_href if next_href.startswith("http") else base_url + next_href


                if not safe_get(driver, full_url):
                    break


                page += 1
                driver.get(full_url)
                time.sleep(2)
            except Exception as e:
                print("âœ… æ‰¾ä¸åˆ°ä¸‹ä¸€é ï¼ŒçµæŸ")
                break

    driver.quit()
    print(f"\nğŸ“„ æ‰€æœ‰çµæœå·²å¯«å…¥ï¼š{output_file}")


if __name__ == "__main__":
    base_url = "https://www.iyp.com.tw"
    initial_url = f"{base_url}/search?q=è‚¡ä»½æœ‰é™å…¬å¸&type=keywords"
    area = select_area_location()
    
    # å»ºç«‹æš«æ™‚çš„ driver ä¾†å–å¾—å€åŸŸ map
    driver_temp = webdriver.Chrome(service=Service(CHROMEDRIVER_PATH), options=options)
    area_map = get_area_map(driver_temp, initial_url)
    driver_temp.quit()

    if area in area_map:
        for city in area_map[area]:
            print(f"ğŸŒ é–‹å§‹æŠ“å–ï¼š{city}")
            Traverse_the_location_data(initial_url, city, base_url)
    else:
        Traverse_the_location_data(initial_url, area, base_url)
