#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Community Data Scraper Module
å¢å¼·ç‰ˆç¤¾å€è³‡æ–™çˆ¬èŸ²æ¨¡çµ„ - æ”¯æ´æ™ºæ…§æª”æ¡ˆç®¡ç†
Contains all the web scraping logic with integrated file management
"""

import requests
from urllib.parse import urljoin, unquote
from bs4 import BeautifulSoup
import time
import re
from pathlib import Path
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import threading
from typing import Callable, Optional, Dict, List, Tuple
import os
from datetime import datetime
import difflib

# åŒ¯å…¥æª”æ¡ˆç®¡ç†å™¨
from file_manager import SmartFileManager


class CommunityDataScraper:
    """
    å¢å¼·ç‰ˆç¤¾å€è³‡æ–™çˆ¬èŸ²é¡åˆ¥
    
    æ–°å¢åŠŸèƒ½ï¼š
    1. æ•´åˆæ™ºæ…§æª”æ¡ˆç®¡ç†å™¨
    2. æ”¯æ´è‡ªå‹•æª”æ¡ˆæ¸…ç†å’Œå‚™ä»½
    3. å¢å¼·çš„éŒ¯èª¤è™•ç†å’Œç‹€æ…‹å›å ±
    4. æ›´éˆæ´»çš„è¼¸å‡ºç®¡ç†
    """
    
    def __init__(self, progress_callback: Optional[Callable] = None, 
                 status_callback: Optional[Callable] = None, 
                 output_folder: Optional[str] = None,
                 file_manager: Optional[SmartFileManager] = None,
                 auto_cleanup: bool = True,
                 enable_backup: bool = True):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆçˆ¬èŸ²
        
        Args:
            progress_callback: é€²åº¦æ›´æ–°å›èª¿å‡½æ•¸
            status_callback: ç‹€æ…‹æ›´æ–°å›èª¿å‡½æ•¸
            output_folder: è¼¸å‡ºè³‡æ–™å¤¾è·¯å¾‘
            file_manager: æª”æ¡ˆç®¡ç†å™¨å¯¦ä¾‹
            auto_cleanup: æ˜¯å¦è‡ªå‹•æ¸…ç†èˆŠæª”æ¡ˆ
            enable_backup: æ˜¯å¦å•Ÿç”¨å‚™ä»½åŠŸèƒ½
        """
        self.base_url = "https://group.lifego.tw/"
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/114.0.0.0 Safari/537.36"
            )
        }
        self.session = self._build_session()
        self.progress_callback = progress_callback
        self.status_callback = status_callback
        self.should_stop = False
        self.output_folder = output_folder or os.getcwd()
        
        # æª”æ¡ˆç®¡ç†è¨­å®š â­ æ–°å¢
        self.auto_cleanup = auto_cleanup
        self.enable_backup = enable_backup
        
        # åˆå§‹åŒ–æª”æ¡ˆç®¡ç†å™¨
        if file_manager:
            self.file_manager = file_manager
        else:
            self.file_manager = SmartFileManager(
                self.output_folder, 
                enable_backup=enable_backup
            )
        
        # çµ±è¨ˆè³‡è¨Š
        self.scrape_stats = {
            'start_time': None,
            'end_time': None,
            'total_communities': 0,
            'new_communities': 0,
            'processed_files': 0,
            'cleaned_files': 0
        }
        
    def _build_session(self) -> requests.Session:
        """å»ºç«‹æ”¯æ´é‡è©¦çš„ Session"""
        session = requests.Session()
        retries = Retry(
            total=5,
            connect=5,
            read=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS", "POST"],
            raise_on_status=False
        )
           
        adapter = HTTPAdapter(max_retries=retries)
        session.mount('https://', adapter)
        session.mount('http://', adapter)
        return session
    
    def _update_status(self, message: str):
        """æ›´æ–°ç‹€æ…‹è¨Šæ¯"""
        if self.status_callback:
            self.status_callback(message)
    
    def _update_progress(self, current: int, total: int):
        """æ›´æ–°é€²åº¦"""
        if self.progress_callback:
            self.progress_callback(current, total)
    
    def stop_scraping(self):
        """åœæ­¢çˆ¬å–"""
        self.should_stop = True
        self._update_status("â¹ï¸ ç”¨æˆ¶è«‹æ±‚åœæ­¢çˆ¬å–")
    
    def get_scrape_statistics(self) -> Dict:
        """å–å¾—çˆ¬å–çµ±è¨ˆè³‡è¨Š"""
        stats = self.scrape_stats.copy()
        if stats['start_time'] and stats['end_time']:
            duration = stats['end_time'] - stats['start_time']
            stats['duration_seconds'] = duration.total_seconds()
            stats['duration_formatted'] = self._format_duration(duration.total_seconds())
        return stats
    
    def _format_duration(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æŒçºŒæ™‚é–“"""
        if seconds < 60:
            return f"{seconds:.1f}ç§’"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.1f}åˆ†é˜"
        else:
            hours = seconds / 3600
            return f"{hours:.1f}å°æ™‚"
    
    def create_city_folder(self, city_name: str, city_count: Optional[str] = None) -> Path:
        """
        å»ºç«‹åŸå¸‚è³‡æ–™å¤¾ - å¢å¼·ç‰ˆ
        
        Args:
            city_name: åŸå¸‚åç¨±
            city_count: åŸå¸‚è³‡æ–™ç¸½ç­†æ•¸
            
        Returns:
            åŸå¸‚è³‡æ–™å¤¾çš„Pathç‰©ä»¶
        """
        if city_count:
            city_data_name = f"{city_name}({city_count}ç­†è³‡æ–™)"
        else:
            city_data_name = city_name
        
        today_str = datetime.now().strftime("%Y_%m_%d")
        today_folder_name = f"{city_data_name}_{today_str}"
        
        base_dir = Path(self.output_folder)
        base_dir.mkdir(parents=True, exist_ok=True)
        
        today_folder_path = base_dir / today_folder_name
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡å‘½åèˆŠè³‡æ–™å¤¾
        escaped_city_name = re.escape(city_data_name)
        pattern = re.compile(rf"^{escaped_city_name}_(\d{{4}}_\d{{2}}_\d{{2}})$")
        
        old_folders = []
        try:
            for entry in base_dir.iterdir():
                if entry.is_dir() and pattern.match(entry.name):
                    if entry.name != today_folder_name:
                        date_match = pattern.match(entry.name)
                        if date_match:
                            date_str = date_match.group(1)
                            try:
                                folder_date = datetime.strptime(date_str, "%Y_%m_%d")
                                old_folders.append((entry, date_str, folder_date))
                            except ValueError:
                                self._update_status(f"âš ï¸ ç„¡æ³•è§£æè³‡æ–™å¤¾æ—¥æœŸ: {date_str}")
        except (OSError, PermissionError) as e:
            self._update_status(f"âš ï¸ è®€å–ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        old_folders.sort(key=lambda x: x[2], reverse=True)
        
        if today_folder_path.exists():
            self._update_status(f"ğŸ“‚ ä»Šå¤©çš„è³‡æ–™å¤¾ {today_folder_name} å·²å­˜åœ¨ï¼Œç¹¼çºŒä½¿ç”¨")
        else:
            if old_folders and self.auto_cleanup:
                # ä½¿ç”¨æª”æ¡ˆç®¡ç†å™¨è™•ç†èˆŠè³‡æ–™å¤¾
                old_folder_path, old_date_str, old_date = old_folders[0]
                try:
                    if self.enable_backup:
                        # å‚™ä»½èˆŠè³‡æ–™å¤¾
                        backup_folder = self.file_manager.backup_folder / old_folder_path.name
                        if backup_folder.exists():
                            timestamp = datetime.now().strftime('%H%M%S')
                            backup_folder = backup_folder.with_name(f"{backup_folder.name}_{timestamp}")
                        
                        backup_folder.parent.mkdir(parents=True, exist_ok=True)
                        old_folder_path.rename(backup_folder)
                        self._update_status(f"ğŸ“¦ å·²å‚™ä»½èˆŠè³‡æ–™å¤¾: {old_folder_path.name} -> å‚™ä»½æª”æ¡ˆ/{backup_folder.name}")
                    
                    # å»ºç«‹æ–°è³‡æ–™å¤¾
                    today_folder_path.mkdir(parents=True, exist_ok=True)
                    self._update_status(f"âœ… å·²å»ºç«‹æ–°è³‡æ–™å¤¾: {today_folder_name}")
                    
                except (OSError, PermissionError) as e:
                    self._update_status(f"âŒ è™•ç†èˆŠè³‡æ–™å¤¾å¤±æ•—: {e}")
                    today_folder_path.mkdir(parents=True, exist_ok=True)
                    self._update_status(f"âœ… å·²å»ºç«‹æ–°è³‡æ–™å¤¾: {today_folder_name}")
            else:
                try:
                    today_folder_path.mkdir(parents=True, exist_ok=True)
                    self._update_status(f"âœ… å·²å»ºç«‹è³‡æ–™å¤¾: {today_folder_name}")
                except (OSError, PermissionError) as e:
                    self._update_status(f"âŒ å»ºç«‹è³‡æ–™å¤¾å¤±æ•—: {e}")
                    return base_dir
        
        return today_folder_path
 
    def separate_name(self, text: str) -> Tuple[str, str]:
        """åˆ†é›¢åç¨±å’Œæ•¸é‡"""
        name_part, sep, num_part = text.partition('(')
        name = name_part.strip()
        count = num_part.rstrip(')').strip()
        return name, count
    
    def get_city_data(self) -> List[Dict]:
        """ç²å–æ‰€æœ‰åŸå¸‚è³‡æ–™"""
        try:
            self._update_status("ğŸŒ æ­£åœ¨é€£æ¥ç›®æ¨™ç¶²ç«™...")
            response = self.session.get(self.base_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            self._update_status("ğŸ“ æ­£åœ¨è§£æåŸå¸‚åˆ—è¡¨...")
            soup = BeautifulSoup(response.text, "html.parser")
            city_tree = soup.find_all('li', {'class': 'treeview'})
            
            cities = []
            total_districts = 0
            
            for tree in city_tree:
                span = tree.find('span')
                if not span:
                    continue
                
                city_text = span.get_text(strip=True)
                city_name, city_count = self.separate_name(city_text)
                
                # ç²å–è©²åŸå¸‚çš„å€åŸŸåˆ—è¡¨
                districts = []
                for city in tree.find_all('ul', {'class': 'treeview-menu'}):
                    for dis in city.find_all('li'):
                        a_tag = dis.find('a')
                        if not a_tag:
                            continue
                        a_text = a_tag.get_text(strip=True)
                        if a_text.startswith('å…¨éƒ¨'):
                            continue
                        district_name, district_count = self.separate_name(a_text)
                        districts.append({
                            'name': district_name,
                            'count': district_count,
                            'url': urljoin(self.base_url, unquote(a_tag['href']))
                        })
                
                # ç²å–åŸå¸‚å…¨éƒ¨è³‡æ–™çš„URL
                city_all_url = None
                for city in tree.find_all('ul', {'class': 'treeview-menu'}):
                    a_tag = city.find('a')
                    if a_tag:
                        city_all_url = urljoin(self.base_url, unquote(a_tag['href']))
                        break
                
                cities.append({
                    'name': city_name,
                    'count': city_count,
                    'districts': districts,
                    'all_url': city_all_url
                })
                
                total_districts += len(districts)
            
            self._update_status(f"âœ… æˆåŠŸè¼‰å…¥ {len(cities)} å€‹åŸå¸‚ï¼Œå…± {total_districts} å€‹å€åŸŸ")
            return cities
            
        except Exception as e:
            self._update_status(f"âŒ ç²å–åŸå¸‚è³‡æ–™å¤±æ•—: {str(e)}")
            return []

    def scrape_new_communities_from_web(self, url: str) -> List[Dict[str, str]]:
        """å¾ç¶²ç«™çˆ¬å–æ‰€æœ‰ç¤¾å€è³‡æ–™"""
        all_communities = []
        
        try:
            page = 1
            
            while not self.should_stop:
                self._update_status(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é ...")
                
                response = self.session.get(url, headers=self.headers, timeout=10)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, "html.parser")
                
                div_tags = soup.find_all('div', {'class': 'product-info'})
                if not div_tags:
                    self._update_status(f"ğŸ“‹ ç¬¬ {page} é æ²’æœ‰æ›´å¤šè³‡æ–™ï¼Œçˆ¬å–å®Œæˆ")
                    break
                    
                page_communities = 0
                for div_tag in div_tags:
                    if self.should_stop:
                        break
                        
                    a_title = div_tag.find('a')
                    if not a_title:
                        continue
                        
                    title = a_title.text.strip().replace('\xa0', '')
                    if title:
                        phone = address = ""
                        spans = div_tag.find('span')
                        if spans:
                            for label in spans.find_all('label'):
                                label_name = label.get_text(strip=True).replace('\xa0', '')
                                label_text = label.next_sibling
                                if not label_text:
                                    continue
                                value = label_text.strip().replace('\xa0', '')
                                if label_name == "é›»è©±":
                                    phone = value
                                elif label_name == "åœ°å€":
                                    address = value
                        
                        all_communities.append({
                            'name': title,
                            'phone': phone,
                            'address': address
                        })
                        page_communities += 1
                
                self._update_status(f"ğŸ“Š ç¬¬ {page} é æ‰¾åˆ° {page_communities} ç­†ç¤¾å€è³‡æ–™")
                
                # å°‹æ‰¾ä¸‹ä¸€é é€£çµ
                a_tag = soup.find('a', class_='btn btn-primary', string=re.compile(r'ä¸‹ä¸€é '))
                href = a_tag and a_tag.get('href')
                if not href:
                    break
                
                url = urljoin(self.base_url, unquote(href))
                page += 1
                time.sleep(2)  # é¿å…è«‹æ±‚éæ–¼é »ç¹
            
            if not self.should_stop:
                self._update_status(f"ğŸ¯ ç¶²ç«™çˆ¬å–å®Œæˆï¼Œå…±å–å¾— {len(all_communities)} ç­†ç¤¾å€è³‡æ–™")
        
        except Exception as e:
            self._update_status(f"âŒ çˆ¬å–ç¶²ç«™è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        
        return all_communities
    
    def parse_existing_communities_from_content(self, content: str) -> Dict[str, Dict[str, str]]:
        """å¾å…§å®¹å­—ä¸²è§£æç¾æœ‰çš„ç¤¾å€è³‡æ–™"""
        communities = {}
        
        if not content.strip():
            return communities
        
        try:
            # åˆ†é›¢æ¨™æº–è³‡æ–™å’Œæ›´æ–°æ—¥èªŒ
            separator_pattern = r'={60,}'
            parts = re.split(separator_pattern, content)
            
            if parts:
                # è§£ææ¨™æº–è³‡æ–™éƒ¨åˆ†
                old_data_content = parts[0].strip()
                if old_data_content:
                    old_communities = self._parse_community_blocks(old_data_content)
                    communities.update(old_communities)
                
                # è§£ææ›´æ–°æ—¥èªŒéƒ¨åˆ†
                if len(parts) > 1:
                    log_content = ''.join(parts[1:])
                    new_communities = self._parse_update_log_communities(log_content)
                    communities.update(new_communities)
            else:
                # æ•´å€‹å…§å®¹éƒ½æ˜¯æ¨™æº–è³‡æ–™
                all_communities = self._parse_community_blocks(content)
                communities.update(all_communities)
        
        except Exception as e:
            self._update_status(f"âŒ è§£æç¾æœ‰å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        
        return communities

    def _parse_community_blocks(self, content: str) -> Dict[str, Dict[str, str]]:
        """è§£ææ¨™æº–æ ¼å¼çš„ç¤¾å€è³‡æ–™å€å¡Š"""
        communities = {}
        
        # ä»¥ç©ºè¡Œåˆ†å‰²æ¯å€‹ç¤¾å€çš„è³‡æ–™
        blocks = re.split(r'\n\s*\n', content.strip())
        
        for block in blocks:
            if not block.strip():
                continue
            
            lines = [line.strip() for line in block.split('\n') if line.strip()]
            if not lines:
                continue
            
            community_name = lines[0]
            phone = ''
            address = ''
            
            # è§£æé›»è©±å’Œåœ°å€
            for line in lines[1:]:
                if ':' in line or 'ï¼š' in line:
                    separator = ':' if ':' in line else 'ï¼š'
                    parts = line.split(separator, 1)
                    if len(parts) == 2:
                        key = parts[0].strip()
                        value = parts[1].strip()
                        
                        if key == 'é›»è©±':
                            phone = value
                        elif key == 'åœ°å€':
                            address = value
            
            if community_name:
                communities[community_name] = {
                    'phone': phone,
                    'address': address
                }
        
        return communities

    def _parse_update_log_communities(self, log_content: str) -> Dict[str, Dict[str, str]]:
        """è§£ææ›´æ–°æ—¥èªŒä¸­çš„ç¤¾å€è³‡æ–™"""
        communities = {}
        
        try:
            lines = log_content.split('\n')
            i = 0
            
            while i < len(lines):
                line = lines[i].strip()
                
                # æ‰¾åˆ°ç¤¾å€åç¨±è¡Œï¼ˆä»¥ "+ " é–‹é ­ï¼‰
                if line.startswith('+ '):
                    community_name = line[2:].strip()
                    phone = ''
                    address = ''
                    
                    # æŸ¥çœ‹æ¥ä¸‹ä¾†çš„è¡Œï¼Œå°‹æ‰¾é›»è©±å’Œåœ°å€
                    j = i + 1
                    while j < len(lines) and j < i + 5:
                        next_line = lines[j].strip()
                        
                        # å¦‚æœé‡åˆ°ä¸‹ä¸€å€‹ç¤¾å€æˆ–å…¶ä»–åˆ†éš”å…§å®¹ï¼Œåœæ­¢
                        if (next_line.startswith('+ ') or 
                            next_line.startswith('ç¸½è¨ˆæ–°å¢é …ç›®') or
                            next_line.startswith('============')):
                            break
                        
                        # è§£æé›»è©±å’Œåœ°å€
                        if 'ğŸ“ é›»è©±:' in next_line:
                            phone = next_line.split('ğŸ“ é›»è©±:', 1)[1].strip()
                        elif 'ğŸ“ åœ°å€:' in next_line:
                            address = next_line.split('ğŸ“ åœ°å€:', 1)[1].strip()
                        
                        j += 1
                    
                    if community_name:
                        communities[community_name] = {
                            'phone': phone,
                            'address': address
                        }
                    
                    i = j
                else:
                    i += 1
        
        except Exception as e:
            self._update_status(f"âŒ è§£ææ›´æ–°æ—¥èªŒæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        
        return communities
    
    def find_data(self, url: str, city_name: str, district_name: Optional[str], 
                  count: str, do_dis: str, city_count: Optional[str] = None) -> bool:
        """
        çˆ¬å–ç¤¾å€è³‡æ–™ - å¢å¼·ç‰ˆå¢é‡æ›´æ–°æ¨¡å¼
        
        Args:
            url: è¦çˆ¬å–çš„URL
            city_name: åŸå¸‚åç¨±
            district_name: å€åŸŸåç¨±ï¼ˆå¯é¸ï¼‰
            count: è³‡æ–™æ•¸é‡
            do_dis: æ˜¯å¦åˆ†å€ï¼ˆ"æ˜¯"æˆ–"å¦"ï¼‰
            city_count: åŸå¸‚ç¸½ç­†æ•¸ï¼ˆå¯é¸ï¼‰

        Returns:
            Trueè¡¨ç¤ºæˆåŠŸï¼ŒFalseè¡¨ç¤ºå¤±æ•—
        """
        try:
            # å»ºç«‹åŸå¸‚è³‡æ–™å¤¾
            city_folder = self.create_city_folder(city_name, city_count)
            
            # ç”Ÿæˆæª”æ¡ˆåç¨±
            today_str = datetime.now().strftime("%Y_%m_%d")
            if do_dis == "å¦":
                file_name = f"{city_name}å…¨éƒ¨ç¤¾å€è³‡æ–™(å…±æœ‰{count}ç­†)_{today_str}.txt"
            elif do_dis == "æ˜¯":
                file_name = f"{city_name}{district_name}ç¤¾å€è³‡æ–™(å…±æœ‰{count}ç­†)_{today_str}.txt"
            
            file_path = city_folder / file_name
            
            self._update_status(f"ğŸ¯ æ­£åœ¨è™•ç†: {file_name}")
            self._update_status(f"ğŸ“ è¼¸å‡ºè·¯å¾‘: {file_path}")
            
            # è§£æç¾æœ‰æª”æ¡ˆä¸­çš„ç¤¾å€è³‡æ–™
            existing_communities = {}
            if file_path.exists():
                with file_path.open('r', encoding='utf-8') as f:
                    content = f.read()
                existing_communities = self.parse_existing_communities_from_content(content)
                self._update_status(f"ğŸ“‹ ç¾æœ‰æª”æ¡ˆä¸­å·²æœ‰ {len(existing_communities)} ç­†ç¤¾å€è³‡æ–™")
            
            # å¾ç¶²ç«™çˆ¬å–æœ€æ–°è³‡æ–™
            self._update_status("ğŸ•·ï¸ é–‹å§‹å¾ç¶²ç«™çˆ¬å–æœ€æ–°è³‡æ–™...")
            all_web_communities = self.scrape_new_communities_from_web(url)
            
            if self.should_stop:
                return False
            
            self._update_status(f"ğŸ“Š å¾ç¶²ç«™çˆ¬å–åˆ° {len(all_web_communities)} ç­†ç¤¾å€è³‡æ–™")
            
            # æ‰¾å‡ºæ–°å¢çš„ç¤¾å€
            new_communities = []
            for community in all_web_communities:
                if community['name'] not in existing_communities:
                    new_communities.append(community)
            
            self._update_status(f"ğŸ†• ç™¼ç¾ {len(new_communities)} ç­†æ–°å¢ç¤¾å€è³‡æ–™")
            
            # å¯«å…¥æª”æ¡ˆ
            success = self._write_community_data_file(
                file_path, all_web_communities, existing_communities, 
                new_communities, file_name
            )
            
            # æ›´æ–°çµ±è¨ˆè³‡è¨Š
            if success:
                self.scrape_stats['total_communities'] += len(all_web_communities)
                self.scrape_stats['new_communities'] += len(new_communities)
                self.scrape_stats['processed_files'] += 1
                
                # è¨˜éŒ„æ›´æ–°æ—¥èªŒ
                if hasattr(self, '_update_log_data'):
                    self._update_log_data[file_name] = new_communities
                else:
                    self._update_log_data = {file_name: new_communities}
            
            if not self.should_stop and success:
                self._update_status(f"âœ… å®Œæˆè™•ç†: {file_name}")
                return True
            else:
                self._update_status("â¹ï¸ è™•ç†å·²åœæ­¢æˆ–å¤±æ•—")
                return False
                
        except Exception as e:
            self._update_status(f"âŒ è™•ç†å¤±æ•—: {str(e)}")
            return False

    def _write_community_data_file(self, file_path: Path, all_web_communities: List[Dict], 
                                  existing_communities: Dict, new_communities: List[Dict], 
                                  file_name: str) -> bool:
        """
        å¯«å…¥ç¤¾å€è³‡æ–™æª”æ¡ˆ
        
        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            all_web_communities: æ‰€æœ‰ç¶²ç«™ç¤¾å€è³‡æ–™
            existing_communities: ç¾æœ‰ç¤¾å€è³‡æ–™
            new_communities: æ–°å¢ç¤¾å€è³‡æ–™
            file_name: æª”æ¡ˆåç¨±
            
        Returns:
            Trueè¡¨ç¤ºæˆåŠŸï¼ŒFalseè¡¨ç¤ºå¤±æ•—
        """
        try:
            with file_path.open(mode="w", encoding="utf-8") as file:
                # å¯«å…¥ç¾æœ‰ç¤¾å€è³‡æ–™ï¼ˆæ¨™æº–æ ¼å¼ï¼‰
                existing_from_web = []
                for community in all_web_communities:
                    if community['name'] in existing_communities:
                        existing_from_web.append(community)
                
                # å¯«å…¥ç¾æœ‰è³‡æ–™
                for community in existing_from_web:
                    file.write(f"{community['name']}\n")
                    file.write(f"é›»è©±: {community['phone']}\n")
                    file.write(f"åœ°å€: {community['address']}\n")
                    file.write("\n")
                
                # å¦‚æœæœ‰æ–°å¢è³‡æ–™ï¼Œæ·»åŠ æ›´æ–°æ—¥èªŒ
                if new_communities:
                    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    file.write("============================================================\n")
                    file.write(f"ğŸ“Š è³‡æ–™æ›´æ–°æ—¥èªŒ - {timestamp}\n")
                    file.write("============================================================\n")
                    file.write(f"ğŸ“ æª”æ¡ˆ: {file_name}\n")
                    file.write(f"ğŸ†• æ–°å¢é …ç›®æ•¸é‡: {len(new_communities)}\n")
                    file.write("----------------------------------------\n")
                    
                    for community in new_communities:
                        file.write(f"+ {community['name']}\n")
                        phone = community.get('phone', '') or 'æœªæä¾›'
                        file.write(f"  ğŸ“ é›»è©±: {phone}\n")
                        address = community.get('address', '') or 'æœªæä¾›'
                        file.write(f"  ğŸ“ åœ°å€: {address}\n")
                        file.write("\n")
                    
                    file.write(f"ğŸ“ˆ ç¸½è¨ˆæ–°å¢é …ç›®: {len(new_communities)} ç­†\n")
                    file.write("============================================================\n")
                    
                    self._update_status(f"âœ… å·²å°‡ {len(new_communities)} ç­†æ–°å¢è³‡æ–™åŠ å…¥æª”æ¡ˆ")
                else:
                    self._update_status("ğŸ“‹ æ²’æœ‰ç™¼ç¾æ–°å¢è³‡æ–™ï¼Œæª”æ¡ˆå·²æ›´æ–°")
            
            return True
            
        except Exception as e:
            self._update_status(f"âŒ å¯«å…¥æª”æ¡ˆå¤±æ•—: {str(e)}")
            return False

    def create_update_log(self, updates: Dict[str, List[Dict[str, str]]]):
        """å»ºç«‹å¢å¼·ç‰ˆæ›´æ–°æ—¥èªŒ"""
        try:
            if not any(updates.values()):
                self._update_status("ğŸ“‹ æ²’æœ‰æ–°å¢é …ç›®ï¼Œä¸å»ºç«‹æ›´æ–°æ—¥èªŒ")
                return
            
            # å»ºç«‹æ›´æ–°æ—¥èªŒè³‡æ–™å¤¾
            log_folder = Path(self.output_folder) / "è³‡æ–™æ›´æ–°æ—¥èªŒ"
            log_folder.mkdir(parents=True, exist_ok=True)
            
            # å»ºç«‹æ›´æ–°æ—¥èªŒæª”æ¡ˆ
            date_str = datetime.now().strftime("%Y-%m-%d")
            log_file_path = log_folder / f"{date_str}è³‡æ–™æ›´æ–°æ—¥èªŒ.txt"
            
            with log_file_path.open('a', encoding='utf-8') as f:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write("=" * 80 + "\n")
                f.write(f"ğŸ“Š ç¤¾å€è³‡æ–™çˆ¬èŸ² - æ›´æ–°æ—¥èªŒ\n")
                f.write(f"ğŸ• æ›´æ–°æ™‚é–“: {timestamp}\n")
                f.write("=" * 80 + "\n\n")
                
                total_new_items = 0
                for file_name, new_communities in updates.items():
                    if new_communities:
                        f.write(f"ğŸ“ æª”æ¡ˆ: {file_name}\n")
                        f.write(f"ğŸ†• æ–°å¢é …ç›®æ•¸é‡: {len(new_communities)}\n")
                        f.write("-" * 60 + "\n")
                        
                        for i, community in enumerate(new_communities, 1):
                            f.write(f"{i:3d}. {community['name']}\n")
                            phone = community.get('phone', '') or 'æœªæä¾›'
                            f.write(f"     ğŸ“ é›»è©±: {phone}\n")
                            address = community.get('address', '') or 'æœªæä¾›'
                            f.write(f"     ğŸ“ åœ°å€: {address}\n")
                            f.write("\n")
                        
                        f.write(f"å°è¨ˆ: {len(new_communities)} ç­†\n")
                        f.write("\n")
                        total_new_items += len(new_communities)
                
                f.write("=" * 80 + "\n")
                f.write(f"ğŸ“ˆ æœ¬æ¬¡æ›´æ–°ç¸½è¨ˆ: {total_new_items} ç­†æ–°å¢è³‡æ–™\n")
                
                # æ·»åŠ çµ±è¨ˆæ‘˜è¦
                stats = self.get_scrape_statistics()
                if stats.get('duration_formatted'):
                    f.write(f"â±ï¸ è™•ç†æ™‚é–“: {stats['duration_formatted']}\n")
                f.write(f"ğŸ“„ è™•ç†æª”æ¡ˆæ•¸: {stats.get('processed_files', 0)}\n")
                
                f.write("=" * 80 + "\n\n")
            
            self._update_status(f"ğŸ“‹ å·²å»ºç«‹æ›´æ–°æ—¥èªŒ: {log_file_path}")
            
        except Exception as e:
            self._update_status(f"âŒ å»ºç«‹æ›´æ–°æ—¥èªŒæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    
    def scrape_all_cities_with_districts(self, cities_data: List[Dict]) -> bool:
        """çˆ¬å–å…¨éƒ¨åŸå¸‚åˆ†å€è³‡æ–™ - å¢å¼·ç‰ˆ"""
        try:
            self.scrape_stats['start_time'] = datetime.now()
            self._update_log_data = {}
            
            # è¨ˆç®—ç¸½å€åŸŸæ•¸é‡
            total_districts = sum(len(city['districts']) for city in cities_data)
            current_district = 0
            
            self._update_status(f"ğŸš€ é–‹å§‹çˆ¬å–å…¨éƒ¨åŸå¸‚åˆ†å€è³‡æ–™ï¼Œå…± {len(cities_data)} å€‹åŸå¸‚ï¼Œ{total_districts} å€‹å€åŸŸ")
            
            for city_index, city in enumerate(cities_data, 1):
                if self.should_stop:
                    break
                    
                self._update_status(f"ğŸ™ï¸ è™•ç†ç¬¬ {city_index}/{len(cities_data)} å€‹åŸå¸‚: {city['name']}")
                self.create_city_folder(city['name'], city['count'])
                
                for district_index, district in enumerate(city['districts'], 1):
                    if self.should_stop:
                        break
                        
                    current_district += 1
                    self._update_progress(current_district, total_districts)
                    
                    self._update_status(f"ğŸ˜ï¸ è™•ç† {city['name']} ç¬¬ {district_index}/{len(city['districts'])} å€‹å€åŸŸ: {district['name']}")
                    
                    success = self.find_data(
                        district['url'], 
                        city['name'], 
                        district['name'], 
                        district['count'], 
                        "æ˜¯",
                        city['count']
                    )
                    if not success:
                        self._update_status(f"âŒ è™•ç† {city['name']}-{district['name']} å¤±æ•—")
                        return False
            
            # è‡ªå‹•æ¸…ç†æª”æ¡ˆï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.auto_cleanup and not self.should_stop:
                self._perform_auto_cleanup()
            
            # å»ºç«‹æ›´æ–°æ—¥èªŒ
            if hasattr(self, '_update_log_data') and not self.should_stop:
                self.create_update_log(self._update_log_data)
            
            self.scrape_stats['end_time'] = datetime.now()
            
            if not self.should_stop:
                stats = self.get_scrape_statistics()
                self._update_status(f"ğŸ‰ å…¨éƒ¨åŸå¸‚åˆ†å€çˆ¬å–å®Œæˆï¼")
                self._update_status(f"ğŸ“Š çµ±è¨ˆ: è™•ç†äº† {stats['processed_files']} å€‹æª”æ¡ˆï¼Œ"
                                  f"ç¸½è¨ˆ {stats['total_communities']} ç­†ç¤¾å€è³‡æ–™ï¼Œ"
                                  f"æ–°å¢ {stats['new_communities']} ç­†ï¼Œ"
                                  f"è€—æ™‚ {stats.get('duration_formatted', 'æœªçŸ¥')}")
                return True
            else:
                self._update_status("â¹ï¸ çˆ¬å–å·²è¢«ç”¨æˆ¶åœæ­¢")
                return False
            
        except Exception as e:
            self.scrape_stats['end_time'] = datetime.now()
            self._update_status(f"âŒ çˆ¬å–å…¨éƒ¨åŸå¸‚åˆ†å€è³‡æ–™å¤±æ•—: {str(e)}")
            return False

    def _perform_auto_cleanup(self):
        """åŸ·è¡Œè‡ªå‹•æª”æ¡ˆæ¸…ç†"""
        try:
            self._update_status("ğŸ§¹ é–‹å§‹åŸ·è¡Œè‡ªå‹•æª”æ¡ˆæ¸…ç†...")
            
            patterns = ["*ç¤¾å€è³‡æ–™*.txt", "*åŸå¸‚*.txt", "*å€åŸŸ*.txt"]
            total_cleaned = 0
            
            for pattern in patterns:
                cleaned_files = self.file_manager.clean_old_files(pattern, keep_latest=1)
                total_cleaned += len(cleaned_files)
            
            if total_cleaned > 0:
                self.scrape_stats['cleaned_files'] = total_cleaned
                self._update_status(f"âœ… è‡ªå‹•æ¸…ç†å®Œæˆï¼Œè™•ç†äº† {total_cleaned} å€‹èˆŠæª”æ¡ˆ")
            else:
                self._update_status("ğŸ“‹ æ²’æœ‰éœ€è¦æ¸…ç†çš„èˆŠæª”æ¡ˆ")
                
        except Exception as e:
            self._update_status(f"âŒ è‡ªå‹•æ¸…ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

    def scrape_single_city_with_districts(self, city_data: Dict) -> bool:
        """çˆ¬å–å–®ä¸€åŸå¸‚åˆ†å€è³‡æ–™ - å¢å¼·ç‰ˆ"""
        try:
            self.scrape_stats['start_time'] = datetime.now()
            self._update_log_data = {}
            
            self._update_status(f"ğŸ™ï¸ é–‹å§‹çˆ¬å– {city_data['name']} çš„åˆ†å€è³‡æ–™")
            self.create_city_folder(city_data['name'], city_data['count'])
            total_districts = len(city_data['districts'])
            
            for i, district in enumerate(city_data['districts']):
                if self.should_stop:
                    break
                    
                self._update_progress(i + 1, total_districts)
                self._update_status(f"ğŸ˜ï¸ è™•ç†ç¬¬ {i+1}/{total_districts} å€‹å€åŸŸ: {district['name']}")
                
                success = self.find_data(
                    district['url'], 
                    city_data['name'], 
                    district['name'], 
                    district['count'], 
                    "æ˜¯",
                    city_data['count']
                )
                if not success:
                    return False
            
            # è‡ªå‹•æ¸…ç†æª”æ¡ˆï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.auto_cleanup and not self.should_stop:
                self._perform_auto_cleanup()
            
            # å»ºç«‹æ›´æ–°æ—¥èªŒ
            if hasattr(self, '_update_log_data') and not self.should_stop:
                self.create_update_log(self._update_log_data)
            
            self.scrape_stats['end_time'] = datetime.now()
            
            if not self.should_stop:
                stats = self.get_scrape_statistics()
                self._update_status(f"ğŸ‰ {city_data['name']} åˆ†å€çˆ¬å–å®Œæˆï¼")
                self._update_status(f"ğŸ“Š çµ±è¨ˆ: è™•ç†äº† {stats['processed_files']} å€‹æª”æ¡ˆï¼Œ"
                                  f"ç¸½è¨ˆ {stats['total_communities']} ç­†ç¤¾å€è³‡æ–™ï¼Œ"
                                  f"æ–°å¢ {stats['new_communities']} ç­†ï¼Œ"
                                  f"è€—æ™‚ {stats.get('duration_formatted', 'æœªçŸ¥')}")
                return True
            else:
                return False
            
        except Exception as e:
            self.scrape_stats['end_time'] = datetime.now()
            self._update_status(f"âŒ çˆ¬å–å–®ä¸€åŸå¸‚åˆ†å€è³‡æ–™å¤±æ•—: {str(e)}")
            return False
    
    def scrape_single_district(self, city_data: Dict, district_name: str) -> bool:
        """çˆ¬å–å–®ä¸€å€åŸŸè³‡æ–™ - å¢å¼·ç‰ˆ"""
        try:
            self.scrape_stats['start_time'] = datetime.now()
            self._update_log_data = {}
            
            self._update_status(f"ğŸ˜ï¸ é–‹å§‹çˆ¬å– {city_data['name']}-{district_name} çš„è³‡æ–™")
            self.create_city_folder(city_data['name'], city_data['count'])
            
            # å°‹æ‰¾æŒ‡å®šçš„å€åŸŸ
            target_district = None
            for district in city_data['districts']:
                if district['name'] == district_name:
                    target_district = district
                    break
            
            if not target_district:
                self._update_status(f"âŒ æ‰¾ä¸åˆ°å€åŸŸ: {district_name}")
                return False
            
            self._update_progress(1, 1)
            
            success = self.find_data(
                target_district['url'], 
                city_data['name'], 
                target_district['name'], 
                target_district['count'], 
                "æ˜¯",
                city_data['count']
            )
            
            # å»ºç«‹æ›´æ–°æ—¥èªŒ
            if hasattr(self, '_update_log_data') and success:
                self.create_update_log(self._update_log_data)
            
            self.scrape_stats['end_time'] = datetime.now()
            
            if success and not self.should_stop:
                stats = self.get_scrape_statistics()
                self._update_status(f"ğŸ‰ {city_data['name']}-{district_name} çˆ¬å–å®Œæˆï¼")
                self._update_status(f"ğŸ“Š çµ±è¨ˆ: è™•ç†äº† {stats['processed_files']} å€‹æª”æ¡ˆï¼Œ"
                                  f"ç¸½è¨ˆ {stats['total_communities']} ç­†ç¤¾å€è³‡æ–™ï¼Œ"
                                  f"æ–°å¢ {stats['new_communities']} ç­†ï¼Œ"
                                  f"è€—æ™‚ {stats.get('duration_formatted', 'æœªçŸ¥')}")
                return True
            else:
                return False
            
        except Exception as e:
            self.scrape_stats['end_time'] = datetime.now()
            self._update_status(f"âŒ çˆ¬å–å–®ä¸€å€åŸŸè³‡æ–™å¤±æ•—: {str(e)}")
            return False


# ============================================================================
# ä½¿ç”¨ç¯„ä¾‹å’Œæ¸¬è©¦
# ============================================================================

def demo_enhanced_scraper():
    """æ¼”ç¤ºå¢å¼·ç‰ˆçˆ¬èŸ²çš„ä½¿ç”¨æ–¹æ³•"""
    print("ğŸ§ª æ¼”ç¤ºå¢å¼·ç‰ˆç¤¾å€è³‡æ–™çˆ¬èŸ²")
    
    def status_callback(message):
        print(f"ğŸ“¢ {message}")
    
    def progress_callback(current, total):
        percentage = (current / total * 100) if total > 0 else 0
        print(f"ğŸ“Š é€²åº¦: {current}/{total} ({percentage:.1f}%)")
    
    # å»ºç«‹å¢å¼·ç‰ˆçˆ¬èŸ²
    scraper = CommunityDataScraper(
        status_callback=status_callback,
        progress_callback=progress_callback,
        output_folder="./æ¸¬è©¦è³‡æ–™",
        auto_cleanup=True,
        enable_backup=True
    )
    
    print("\nğŸŒ è¼‰å…¥åŸå¸‚è³‡æ–™:")
    cities_data = scraper.get_city_data()
    
    if cities_data:
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(cities_data)} å€‹åŸå¸‚è³‡æ–™")
        
        # æ¸¬è©¦çˆ¬å–å–®ä¸€åŸå¸‚çš„ç¬¬ä¸€å€‹å€åŸŸ
        if cities_data[0]['districts']:
            city = cities_data[0]
            district = city['districts'][0]
            
            print(f"\nğŸ˜ï¸ æ¸¬è©¦çˆ¬å–: {city['name']}-{district['name']}")
            success = scraper.scrape_single_district(city, district['name'])
            
            if success:
                print("âœ… æ¸¬è©¦çˆ¬å–æˆåŠŸ")
                
                # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
                stats = scraper.get_scrape_statistics()
                print(f"\nğŸ“Š çµ±è¨ˆè³‡è¨Š:")
                for key, value in stats.items():
                    print(f"  {key}: {value}")
                
                # é¡¯ç¤ºæª”æ¡ˆç®¡ç†å™¨çµ±è¨ˆ
                file_stats = scraper.file_manager.get_comprehensive_statistics()
                print(f"\nğŸ“ æª”æ¡ˆçµ±è¨ˆ:")
                print(f"  ç¸½æª”æ¡ˆæ•¸: {file_stats['summary']['total_files']}")
                print(f"  ç¸½å¤§å°: {file_stats['summary']['total_size_mb']} MB")
                print(f"  å„²å­˜å¥åº·: {file_stats['summary']['storage_health']}")
            else:
                print("âŒ æ¸¬è©¦çˆ¬å–å¤±æ•—")
    else:
        print("âŒ è¼‰å…¥åŸå¸‚è³‡æ–™å¤±æ•—")


if __name__ == "__main__":
    demo_enhanced_scraper()