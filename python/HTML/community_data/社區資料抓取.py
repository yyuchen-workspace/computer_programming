import requests
from urllib.parse import urljoin, unquote  #é…åˆçµåˆURL
from bs4 import BeautifulSoup
import time
import re #ç”¨åˆ°re.compileè™•ç†ä¸‹ä¸€é 
from pathlib import Path  #è™•ç†è·¯å¾‘
from requests.adapters import HTTPAdapter  #è™•ç†é€£ç·š
from urllib3.util.retry import Retry  #è™•ç†é€£ç·š


base_url = "https://group.lifego.tw/" 
headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/114.0.0.0 Safari/537.36"
        )
    }


# === å»ºç«‹ä¸€æ”¯æ”¯æŒé‡è©¦çš„ Session ===
def build_session() -> requests.Session:
    session = requests.Session()
    retries = Retry(
        total=5,                      # ç¸½å…±é‡è©¦ 5 æ¬¡
        backoff_factor=1,             # æ¯æ¬¡é‡è©¦é–“éš”ï¼š1sã€2sã€4sã€â€¦
        status_forcelist=[429, 500, 502, 503, 504],   # å“ªäº› HTTP ç‹€æ…‹è¦é‡è©¦
        raise_on_status=False
    )
    adapter = HTTPAdapter(max_retries=retries)
    session.mount('https://', adapter)
    session.mount('http://', adapter)
    return session


SESSION = build_session()



def create_city_folder(city_name):
    # 2. å»ºç«‹å®Œæ•´çš„æª”æ¡ˆè·¯å¾‘ç‰©ä»¶
    #    Path(city_name)     â†’ æ–°åŒ—å¸‚
    #    / "data"            â†’ æ–°åŒ—å¸‚/data
    #    / f"{district_name}.txt" â†’ æ–°åŒ—å¸‚/data/ä¸­å±±å€.txt
    city_folder = Path(city_name)
    city_folder.mkdir(parents=True, exist_ok=True)


# çˆ¬å–ç¤¾å€è³‡æ–™
def find_data(url, city_name, district_name, count, do_dis):
    if do_dis == "å¦":
        file_path = Path(city_name) / f"{city_name}å…¨éƒ¨ç¤¾å€è³‡æ–™(å…±æœ‰{count}ç­†).txt"
    elif do_dis == "æ˜¯":
        file_path = Path(city_name) / f"{city_name}{district_name}ç¤¾å€è³‡æ–™(å…±æœ‰{count}ç­†).txt"

    with file_path.open( mode = "w", encoding = "utf-8") as file:
        page = 1
        while True:
            print(f"ğŸ” ç¬¬{page}é ")
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            div_tags = soup.find_all('div', {'class' : 'product-info'})
            for div_tag in div_tags:
                a_title = div_tag.find('a')#æ‰¾åˆ°<a>æ¨™ç±¤
                title = a_title.text.strip().replace('\xa0', '')
                if title != None:
                    file.write(f"{title}\n")#å¯«å…¥æ¨™é¡Œ
                    phone = address = ""#é¿å…ç©ºç™½æˆ–ç„¡è³‡æ–™
                    spans = div_tag.find('span')#æ‰¾åˆ°<span>æ¨™ç±¤
                    for label in spans.find_all('label'):#æ‰¾åˆ°label
                        label_name = label.get_text(strip=True).replace('\xa0','')#å–å¾—labelå…§æ–‡å­—#å»é™¤&nbsp;ï¼ˆä¸æ›è¡Œç©ºæ ¼ï¼‰
                        label_text = label.next_sibling
                        if not label_text:#ç©ºç™½æˆ–æ›è¡Œ
                            continue
                        value = label_text.strip().replace('\xa0', '')#å»é™¤&nbsp;ï¼ˆä¸æ›è¡Œç©ºæ ¼)ã€é ­å°¾ç©ºç™½
                        if label_name == "é›»è©±":
                            phone = value
                        elif label_name == "åœ°å€":
                            address = value
                    file.write(f"é›»è©±: {phone}\nåœ°å€: {address}\n\n")
        
            a_tag = soup.find('a',class_='btn btn-primary',string=re.compile(r'ä¸‹ä¸€é '))
            '''
            if a_tag and a_tag.get('href') != None:
                next_page_url = base_url + a_tag['href']
            else:
                break
            '''
            #urljoinå¥—ä»¶
            href = a_tag and a_tag.get('href')#åˆ©ç”¨andé¿å…a_tagç‚ºNone
            if not href:
                print("å·²ç„¡ä¸‹ä¸€é ï¼ŒçµæŸ")
                break
            url = urljoin(base_url, unquote(href))#æ›´å®Œæ•´ï¼Œå¯ä»¥è‡ªå‹•çµåˆé€£çµ
            #
            page+=1
            time.sleep(2)


def seperate_name(text):
    name_part, sep, num_part = text.partition('(')#å¦‚"å°åŒ—å¸‚ (4776)"
    name = name_part.strip()               # æ‹¿åˆ°ã€Œå°åŒ—å¸‚ã€
    count = num_part.rstrip(')').strip() #æ‹¿åˆ°4776
    return name, count

if __name__ == "__main__":
    response = requests.get(base_url, headers=headers, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    city_tree = soup.find_all('li', {'class' : 'treeview'})#å…¨åœ‹ç¸£å¸‚æ¨¹
    found_city = found_district = False#ç”¨æ–¼å¾Œé¢ç¢ºèªæ˜¯å¦æŸ¥æ‰¾åˆ°è©²ç¸£å¸‚ã€å€å
    city_name = input("è«‹è¼¸å…¥ç¸£å¸‚å: ").strip()
    if city_name == "å…¨éƒ¨":
        found_city = True
        found_district = True
        do_dis = input("æ˜¯å¦åˆ†å€: ").strip()
        while True:
            if do_dis == "æ˜¯":#å…¨éƒ¨ç¸£å¸‚-å…¨å€åˆ†é¡  
                for tree in city_tree:
                    #å–å‡º <span> å…§çš„åŸå¸‚åç¨±
                    span = tree.find('span')
                    if not span:
                        continue

                    city_text = span.get_text(strip = True)
                    city_n, _ = seperate_name(city_text)# æ‹¿åˆ°ã€Œå°åŒ—å¸‚ã€
                    create_city_folder(city_n)
                    #è™•ç†æ­¤åŸå¸‚ä¸‹çš„æ‰€æœ‰å­é¸å–®ï¼ˆul.treeview-menuï¼‰
                    for city in tree.find_all('ul', {'class' : 'treeview-menu'}):
                        for dis in city.find_all('li'):#éæ­·å­é¸å–®
                            a_tag = dis.find('a')
                            a_text = a_tag.get_text(strip=True)#<a>æ¨™ç±¤ä¸‹çš„å­—ä¸²#å…¨éƒ¨æˆ–æ˜¯å€å(ç­†æ•¸)
                            if a_text.startswith('å…¨éƒ¨'):#é‡åˆ°å…¨éƒ¨å°±è·³é
                                continue
                            district_n, count = seperate_name(a_text)                               
                            href = a_tag['href']
                            url =  urljoin(base_url, unquote(href))  #çµåˆé€£çµ
                            find_data(url, city_n, district_n, count, do_dis)
                break
            elif do_dis == "å¦":#å…¨éƒ¨ç¸£å¸‚-å…¨å€ä¸åˆ†é¡
                #é€ä¸€è™•ç†æ¯å€‹åŸå¸‚ç¯€é»
                for tree in city_tree:
                    #å–å‡º <span> å…§çš„åŸå¸‚åç¨±
                    span = tree.find('span')
                    if not span:
                        continue
                    city_text = span.get_text(strip = True)
                    city_n, count = seperate_name(city_text)
                    create_city_folder(city_n)
                    #è™•ç†æ­¤åŸå¸‚ä¸‹çš„æ‰€æœ‰å­é¸å–®ï¼ˆul.treeview-menuï¼‰
                    for city in tree.find_all('ul', {'class' : 'treeview-menu'}):
                        a_tag = city.find('a')#åªæ‰¾ç¬¬ä¸€å€‹(å…¨éƒ¨)
                        href = a_tag['href']
                        url =  urljoin(base_url, unquote(href))  
                        find_data(url, city_n, None, count, do_dis)             
                break
            do_dis = input("éŒ¯èª¤è¼¸å…¥!è«‹è¼¸å…¥æ˜¯/å¦:").strip()
    else:
        district_name = input("è«‹è¼¸å…¥é„‰é®å¸‚å€å: ").strip()
        if district_name == "å…¨éƒ¨":
            found_district = True
            do_dis = input("æ˜¯å¦åˆ†å€: ").strip()
            while True:
                if do_dis == "æ˜¯":#å–®ä¸€ç¸£å¸‚-å…¨å€åˆ†é¡
                    for tree in city_tree:
                        #å–å‡º <span> å…§çš„åŸå¸‚åç¨±
                        span = tree.find('span')
                        if not span:
                            continue

                        city_text = span.get_text(strip = True)
                        city_n, _ = seperate_name(city_text)# æ‹¿åˆ°ã€Œå°åŒ—å¸‚ã€
                        if city_n == city_name:                           
                            found_city == True
                            create_city_folder(city_n)
                            #è™•ç†æ­¤åŸå¸‚ä¸‹çš„æ‰€æœ‰å­é¸å–®ï¼ˆul.treeview-menuï¼‰
                            for city in tree.find_all('ul', {'class' : 'treeview-menu'}):
                                for dis in city.find_all('li'):#éæ­·å­é¸å–®
                                    a_tag = dis.find('a')
                                    a_text = a_tag.get_text(strip=True)#<a>æ¨™ç±¤ä¸‹çš„å­—ä¸²#å…¨éƒ¨æˆ–æ˜¯å€å(ç­†æ•¸)
                                    if a_text.startswith('å…¨éƒ¨'):#é‡åˆ°å…¨éƒ¨å°±è·³é
                                        continue
                                    district_n, count = seperate_name(a_text)                               
                                    href = a_tag['href']
                                    url =  urljoin(base_url, unquote(href))  #çµåˆé€£çµ
                                    find_data(url, city_n, district_n, count, do_dis) 
                    break                  
                elif do_dis == "å¦":#å–®ä¸€ç¸£å¸‚-å…¨å€ä¸åˆ†é¡
                    #é€ä¸€è™•ç†æ¯å€‹åŸå¸‚ç¯€é»
                    for tree in city_tree:
                        #å–å‡º <span> å…§çš„åŸå¸‚åç¨±
                        span = tree.find('span')
                        if not span:
                            continue
                        city_text = span.get_text(strip = True)
                        city_n, count = seperate_name(city_text)
                        if city_n == city_name:
                            found_city == True
                            create_city_folder(city_n)
                            #è™•ç†æ­¤åŸå¸‚ä¸‹çš„æ‰€æœ‰å­é¸å–®ï¼ˆul.treeview-menuï¼‰
                            for city in tree.find_all('ul', {'class' : 'treeview-menu'}):
                                a_tag = city.find('a')#åªæ‰¾ç¬¬ä¸€å€‹(å…¨éƒ¨)
                                href = a_tag['href']
                                url =  urljoin(base_url, unquote(href))  
                                find_data(url, city_n, None, count, do_dis)             
                    break
                do_dis = input("éŒ¯èª¤è¼¸å…¥!è«‹è¼¸å…¥æ˜¯/å¦:").strip()

        else:  #å–®ä¸€ç¸£å¸‚-å–®ä¸€å€
            for tree in city_tree:
                #å–å‡º <span> å…§çš„åŸå¸‚åç¨±
                span = tree.find('span')
                if not span:
                    continue

                city_text = span.get_text(strip = True)
                city_n, _ = seperate_name(city_text)# æ‹¿åˆ°ã€Œå°åŒ—å¸‚ã€
                if city_n == city_name:
                    found_city == True
                #è™•ç†æ­¤åŸå¸‚ä¸‹çš„æ‰€æœ‰å­é¸å–®ï¼ˆul.treeview-menuï¼‰
                    for city in tree.find_all('ul', {'class' : 'treeview-menu'}):
                    
                        for dis in city.find_all('li'):#éæ­·å­é¸å–®
                            a_tag = dis.find('a')
                            a_text = a_tag.get_text(strip=True)#<a>æ¨™ç±¤ä¸‹çš„å­—ä¸²#å…¨éƒ¨æˆ–æ˜¯å€å(ç­†æ•¸)
                            dis_n, count = seperate_name(a_text)   
                            if dis_n == district_name:
                                found_district == True
                                create_city_folder(city_n)
                                href = a_tag['href']
                                url =  urljoin(base_url, unquote(href))  #çµåˆé€£çµ
                                find_data(url, city_n, dis_n, count, "æ˜¯")
                else:
                    continue

        if not found_city:
            print("æŸ¥ç„¡æ­¤ç¸£å¸‚è³‡æ–™æˆ–è¼¸å…¥ç¸£å¸‚åç¨±éŒ¯èª¤")
        elif not found_district:
            print("æŸ¥ç„¡æ­¤é„‰é®å¸‚å€è³‡æ–™æˆ–è¼¸å…¥é„‰é®å¸‚å€åç¨±éŒ¯èª¤")

            









